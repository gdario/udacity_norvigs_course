{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 3 - Introduction - Language\n",
    "\n",
    "Python has a syntax for statements, expressions, classes with operator overloading etc. It also contains some mini-languages like string formatting and regular expressions. These are small, highly specialized languages that serve a specific purpose. The basic idea in this unit is that one can develop a domain-specific language to represent and solve specific problems. To this end, we will describe what a *language* is, what a *grammar* is, the difference between a *compiler* and an *interpreter*, and how to use languages as a design tool.\n",
    "\n",
    "## Regular Expressions\n",
    "\n",
    "REs are an example of a language. It can be expressed by strings like `'a*b*c*'`. To make sense of inputs like these we need to make sense of what the possible *grammars* are and what are the possible *languages* that those grammars correspond to.\n",
    "\n",
    "A **grammar** is a description of a language and a **language** is a set of strings. In our example above, `'a*b*c*` is the description of a grammar and strings like `abc`, `aaabbcc`, `ccccc` form the language associated with such grammar.\n",
    "\n",
    "Representing grammars as strings, like `'a*b*c*` or `'a+b?` is convenient for small expressions, but it can become complex with longer ones. We are going to use a representation that is more **compositional**. We are going to describe an **Application Programming Interface (API)**, (i.e., what the programmer uses, as opposed to the UI, which is what the user uses), i.e., a series of function calls that can be used to describe the grammar of a RE. Python has a `re` module, so we could leverage the functions in that module, but the point of this unit is not to learn how to use REs, but rather how to build a **language processor**.\n",
    "\n",
    "The **API calls** listed below are the building blocks of our language description, i.e., of our grammar.\n",
    "\n",
    "- `lit(s)` is the **literal** string `s`. `lit('a')` describes the language consisting only of character string `'a'`, i.e., `{'a'}`, and nothing else.\n",
    "- `seq(x, y)` is the **sequence** of x and y, meaning the application of RE `y` on what is returned by the application of `x` to the input. `seq(lit('a'), lit('b'))` describes the language consisting only of the string `'ab'`, i.e. `{'ab'}`.\n",
    "- `alt(x, y)` stands for **alternatives**. `seq(lit('a'), lit('b'))` would correspond of two possibilities: either `a` or `b`, i.e., `{'a', 'b'}`. The output is what we would get by applying `x` *or* `y` to the input.\n",
    "- `star(x)` means **zero or more repetitions** of `'x'`, and would therefore correspond to `{'a', 'aa', 'aaa',...}` and so on.\n",
    "- `oneof(s)` is the same as `alt(c1, c2, ...)` where `'c1'. 'c2', ...` etc. are the characters string `s` is made of. `oneof('abc')` matches `{'a', 'b', 'c'}`. Note that the input of `oneof()` is a string of characters, and not other metacharacters (**TODO** confirm that this is true).\n",
    "- `eol` means **end of line* matches only the end of a character string and nothing else, so it matches the empty string `{''}`, but only at the end. `seq(lit('a'), eol)` matches `{'a'}` if it is at the end of the line (it would be the equivalent of `'a$'`).\n",
    "- `dot` matches any possible character `{'a', 'b', 'c',...}`\n",
    "- `plus(s)` means **one or more repetitions** of `'x'`. **NOTE** it does not seem to be implemented in this unit.\n",
    "\n",
    "The API calls above implement a subset of regular expression metacharacters. These are based on [Rob Pike's regular expression matcher](https://www.cs.princeton.edu/courses/archive/spr09/cos333/beautiful.html). The operators above define patterns that are searched for by these two functions:\n",
    "\n",
    "- `search(pattern, text)`: returns a string that is the earliest match of the pattern in the text, and if there is more than one match in the same location, it will be the longest of those.\n",
    "- `match(pattern, text)`: matches only if the pattern occurs at the very start of `text`, so `match('def', 'abcdef')` would return `None`.\n",
    "\n",
    "The naming of these functions is different from that used by Pike, but is consistent with the naming used in the `re` module. Below we provide the implementations of `search()` and `match()`, plus a couple of utility functions. Of particular interest is the structure of `match_star(p, pattern, text)`, which uses a clever recursive call.\n",
    "\n",
    "`match1(p, text)` function returns `True` if the first character of `text` is `p` *or* if `p` is `.`, i.e., if the first character is supposed to be *any* character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match1(p, text):\n",
    "    \"\"\"Return True if first character of text matches pattern character p.\"\"\"\n",
    "    if not text:\n",
    "        return False\n",
    "    return p == '.' or p == text[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`search(pattern, text)` returns `True` if `pattern` is found anywhere in `text`. If the first character of `pattern` is `^`, `pattern` must appear at the very beginning of `text`. Note that this function depends on the function `match(pattern, text)` defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(pattern, text):\n",
    "    \"Return True if pattern appears anywhere in text.\"\n",
    "    if pattern.startswith('^'):\n",
    "        return match(pattern[1:], text)\n",
    "    else:\n",
    "        return match('.*' + pattern, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`match(pattern, text)` returns `True` if `pattern` appears at the start of the in the text. It covers the following special cases:\n",
    "\n",
    "- If `pattern` is the empty string `''` it returns `True`. This is the same behaviour as `re.match()`.\n",
    "- If `pattern` is `'$'` it returns `True` only when the end and the beginning of the string coincide, which only happens for the empty string.\n",
    "- If `pattern` is a multicharacter string and the *second* character is either `'*'` or `'?'`, it splits the pattern into three pieces: the first character `p`, the operator `op`, and the pattern `pat`.\n",
    "  - If `op` is `'*'` it calls `match_star(p, pat, text)`.\n",
    "  - If `op` is `'?'` (zero or one occurrances of the character before) it checks whether `p` matches the first character of `text` and whether `pat` matches the rest of `text`. If this is the case, it returns `True`. If this condition is `False`, it checks whether `pat` matches (all of) `text`.\n",
    "- Finally, if none of the above holds, it makes a recursive call where it checks whether the first character of `pattern` matches the first character of `text` with `match()` and uses (recursively) `match(pattern[1:], test[1:])` to match the rest of the pattern against the rest of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match(pattern, text):\n",
    "    \"Return True if pattern appears at the start of text.\"\n",
    "    if pattern == '':\n",
    "        return True\n",
    "    elif pattern == '$':\n",
    "        return (text == '')\n",
    "    elif len(pattern) > 1 and pattern[1] in '*?':\n",
    "        p, op, pat = pattern[0], pattern[1], pattern[2:]\n",
    "        if op == '*':\n",
    "            return match_star(p, pat, text)\n",
    "        elif op == '?':\n",
    "            if match1(p, text) and match(pat, text[1:]):\n",
    "                return True\n",
    "            else:\n",
    "                return match(pat, text)\n",
    "        else:\n",
    "            return (match1(pattern[0], text) and\n",
    "                    match(pattern[1:], text[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`match_star(p, pattern, text)` returns `True` if zero or more instances of `p` are followed by `pattern`. As above, matching an arbitrary number of instances is achieved via a recursive call to `match_star()` itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_star(p, pattern, text):\n",
    "    \"\"\"Return True if any number of char p, followed by pattern,\n",
    "    matches text.\"\"\"\n",
    "    return (match(pattern, text) or  # match zero times\n",
    "            (match1(p, text) and     # match exactly one time\n",
    "             match_star(p, pattern, text[1:])))  # Brilliant!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Inventory\n",
    "\n",
    "Let's make a list of the concept we need to consider in this unit:\n",
    "\n",
    "- Patterns.\n",
    "- Texts we want to match the patterns against, and the result of this mathing.\n",
    "\n",
    "This is pretty much it, however, we will also consider the following concepts:\n",
    "\n",
    "- A concept of **partial result**.\n",
    "- A notion of **control over iteration**.\n",
    "\n",
    "To understand why these additional notions are needed, let's consider the following example: our pattern is `'a*b+'` and our text is `'aaab'`. If our pattern is represented in our API as `seq(star('a'), plus(lit('b')))`, the first part, `star('a')` would match the first character, but the second part of the pattern, `'b+'` would not match. Only after matching the first three `'a'`s we finally match a `'b'`. We would then need a mechanism to iterate through all possible substrings matching  the first of the pattern, and this seems quite tricky. A similar situation occurs, if we need to evaluate between alternatives. In such cases we need some form of control over this form of iteration.\n",
    "\n",
    "It turns out (no explanation provided in the lesson) that representing these partial results as a **set of remainders of the text** is a good choice. By *remainder* we mean everything left after matching `pattern`. For example, if `pattern = '^a'` and `text = 'abacus'`, the remainder is `'bacus'`. The `matchset(pattern, text)` function below returns this set of remainders. In the case of `star(lit(a))`, `matchset(star(lit(a)), text='aaab')` would then return `set(['aaab', 'aab', 'ab', 'b'])`.\n",
    "\n",
    "To implement `matchset(pattern, text)` we first introduce a utility function `components(pattern)` which breaks a pattern into three parts: the operator `op` and the arguments `x` and `y`, which will be `None` if missing.\n",
    "\n",
    "**Important**: here `pattern` is a *tuple* of the form `(op, x, y)`, for example `('lit', 'abc')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "null = frozenset()  # Acts as Null\n",
    "\n",
    "def components(pattern):\n",
    "    \"\"\"Return the op, x, and y arguments; x and y are None if missing.\n",
    "    pattern is a tuple (op, x, y) with x, y optional.\"\"\"\n",
    "    x = pattern[1] if len(pattern) > 1 else None\n",
    "    y = pattern[2] if len(pattern) > 2 else None\n",
    "    return pattern[0], x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------\n",
    "# User Instructions\n",
    "#\n",
    "# The function, matchset, takes a pattern and a text as input\n",
    "# and returns a set of remainders. For example, if matchset \n",
    "# were called with the pattern star(lit(a)) and the text \n",
    "# 'aaab', matchset would return a set with elements \n",
    "# {'aaab', 'aab', 'ab', 'b'}, since a* can consume none, one, two\n",
    "# or all three of the a's in the text.\n",
    "#\n",
    "# dot:   matches any character.\n",
    "# oneof: matches any of the characters in the string it is \n",
    "#        called with. oneof('abc') will match a or b or c.\n",
    "\n",
    "def matchset(pattern, text):\n",
    "    \"Match pattern at start of text; return a set of remainders of text.\"\n",
    "    op, x, y = components(pattern)\n",
    "    if 'lit' == op:\n",
    "        return set([text[len(x):]]) if text.startswith(x) else null\n",
    "    elif 'seq' == op:\n",
    "        return set(t2 for t1 in matchset(x, text) for t2 in matchset(y, t1))\n",
    "    elif 'alt' == op:\n",
    "        return matchset(x, text) | matchset(y, text)\n",
    "    elif 'dot' == op:\n",
    "        return set([text[1:]])\n",
    "    elif 'oneof' == op:\n",
    "        return set([text[text.find(x)+1:]])\n",
    "    elif 'eol' == op:\n",
    "        return set(['']) if text == '' else null\n",
    "    elif 'star' == op:\n",
    "        return (set([text]) |\n",
    "                set(t2 for t1 in matchset(x, text)\n",
    "                    for t2 in matchset(pattern, t1) if t1 != text))\n",
    "    else:\n",
    "        raise ValueError('unknown pattern: %s' % pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `op == 'alt'` the function returns the result of `matchset(x, text)` or `matchset(y, text)`. This to account for the fact that `x` and `y` may be composite expressions. The result is, therefore, the *union* of these two sets.\n",
    "\n",
    "Let's try to understand the case for `'seq'`. Given `('seq', x, y)`, the expression\n",
    "\n",
    "```python\n",
    "set(t2 for t1 in matchset(x, text) for t2 in matchset(y, t1))\n",
    "```\n",
    "\n",
    "`matchset(x, text)` returns a set the elements of which, indicated by `t1`, become the inputs of `matchset(y, t1)`, which in turn returns a set the elements of which we indicate with `t2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests pass\n"
     ]
    }
   ],
   "source": [
    "def test():\n",
    "    assert matchset((\"lit\", \"abc\"), \"abcdef\") == set([\"def\"])\n",
    "    assert matchset(\n",
    "        (\"seq\", (\"lit\", \"hi \"), (\"lit\", \"there \")), \"hi there nice to meet you\"\n",
    "    ) == set([\"nice to meet you\"])\n",
    "    assert matchset((\"alt\", (\"lit\", \"dog\"), (\"lit\", \"cat\")), \"dog and cat\") == set(\n",
    "        [\" and cat\"]\n",
    "    )\n",
    "    assert matchset((\"dot\",), \"am i missing something?\") == set(\n",
    "        [\"m i missing something?\"]\n",
    "    )\n",
    "    assert matchset((\"oneof\", \"a\"), \"aabc123\") == set([\"abc123\"])\n",
    "    assert matchset((\"eol\",), \"\") == set([\"\"])\n",
    "    assert matchset((\"eol\",), \"not end of line\") == frozenset([])\n",
    "    assert matchset((\"star\", (\"lit\", \"hey\")), \"heyhey!\") == set(\n",
    "        [\"!\", \"heyhey!\", \"hey!\"]\n",
    "    )\n",
    "    return \"tests pass\"\n",
    "\n",
    "print(test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search and Match\n",
    "\n",
    "With this definition of `matchset()` we can implement `search(pattern, text)` and `match(pattern, text)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests pass\n"
     ]
    }
   ],
   "source": [
    "null = frozenset()\n",
    "\n",
    "def search(pattern, text):\n",
    "    \"Match pattern anywhere in text; return longest earliest match or None.\"\n",
    "    for i in range(len(text)):\n",
    "        m = match(pattern, text[i:])\n",
    "        if m:\n",
    "            return m\n",
    "        \n",
    "def match(pattern, text):\n",
    "    \"Match pattern against start of text; return longest match found or None.\"\n",
    "    remainders = matchset(pattern, text)\n",
    "    if remainders:\n",
    "        shortest = min(remainders, key=len)\n",
    "        return text[:text.find(shortest)] if len(text) > 1 else text\n",
    "\n",
    "def test():\n",
    "    assert match(('star', ('lit', 'a')),'aaabcd') == 'aaa'\n",
    "    assert match(('alt', ('lit', 'b'), ('lit', 'c')), 'ab') == None\n",
    "    assert match(('alt', ('lit', 'b'), ('lit', 'a')), 'ab') == 'a'\n",
    "    assert search(('alt', ('lit', 'b'), ('lit', 'c')), 'ab') == 'b'\n",
    "    return 'tests pass'\n",
    "\n",
    "print(test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filling Out The API\n",
    "\n",
    "In the code below we provide the implementations of the various API calls. When we call an operator as a function, say `lit('hello')`, we obtain a tuple containing the name of the operator and, depending on the operator, the arguments or other operator-arguments tuples, e.g., `('lit', 'hello)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests pass\n"
     ]
    }
   ],
   "source": [
    "def lit(string):\n",
    "    return ('lit', string)\n",
    "\n",
    "def seq(x, y):\n",
    "    return ('seq', x, y)\n",
    "\n",
    "def alt(x, y):\n",
    "    return ('alt', x, y)\n",
    "\n",
    "def star(x):\n",
    "    return ('star', x)\n",
    "\n",
    "def plus(x):\n",
    "    return ('seq', x, ('star', x))\n",
    "\n",
    "def opt(x):\n",
    "    return alt(lit(''), x) #opt(x) means that x is optional\n",
    "\n",
    "def oneof(chars):\n",
    "    return ('oneof', tuple(chars))\n",
    "\n",
    "dot = ('dot',)\n",
    "eol = ('eol',)\n",
    "\n",
    "def test():\n",
    "    assert lit('abc') == ('lit', 'abc')\n",
    "    assert seq(('lit', 'a'), ('lit', 'b')) == ('seq', ('lit', 'a'), ('lit', 'b'))\n",
    "    assert alt(('lit', 'a'), ('lit', 'b')) == ('alt', ('lit', 'a'), ('lit', 'b'))\n",
    "    assert star(('lit', 'a')) == ('star', ('lit', 'a'))\n",
    "    assert plus(('lit', 'c')) == ('seq', ('lit', 'c'), ('star', ('lit', 'c')))\n",
    "    assert opt(('lit', 'x')) == ('alt', ('lit', ''), ('lit', 'x'))\n",
    "    assert oneof('abc') == ('oneof', ('a', 'b', 'c'))\n",
    "    return 'tests pass'\n",
    "\n",
    "print(test())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling\n",
    "\n",
    "Let's summarize how *interpreters* work. In the case of REs we have *patterns*, e.g., `(a|b)+`, and we have *languages*, i.e., set of strings like `{'a', 'b', 'ab', 'ba', ...}` defined by the pattern, and then we have **interpreters** like `matchset(pattern, text)` which return a set of strings. We say that `matchset` is an interpreter because it takes a pattern as a data structure and operates over that pattern. As we can see from its implementation, it has a big `if-elif-else` statement where it checks what type of operator we have in order to select the next action.\n",
    "\n",
    "There is an inherent inefficiency, in that the pattern is only defined once, but we may want to apply that same pattern to many different texts. Every time we have to go through the sequence of `if-elif-else` in order to figure out what type of operator we have, but we should already know that.\n",
    "\n",
    "There is another type of interpreter, called the **compiler**, which does all this work at once, the very first time the pattern is defined. Whereas an interpreter takes a pattern and a text and operates on those, a compiler has two steps. In the first step there is a compilation function which takes just the pattern and returns a *compiled object*, let's call it `c`. Then the compiled object is executed taking the text as argument: `c(text)`. While in the interpreter all the work is done by the interpreter itself, in our case by `matchset()`, in the case of a compiler some of the work is done during the compilation stage and some happens every time we get a new text.\n",
    "\n",
    "In the case of `lit` our API returns:\n",
    "\n",
    "```python\n",
    "def lit(s):\n",
    "    return ('lit', 's')\n",
    "```\n",
    "\n",
    "We defined `matchset(pattern, text)` such that when the pattern contains `'lit'` we compute the remainders as:\n",
    "\n",
    "```python\n",
    "def matchset(pattern, text):\n",
    "    op, x, y = components(pattern)\n",
    "    # other code\n",
    "    if 'lit' == op:\n",
    "        return set([text[len(x):]]) if text.startswith(x) else null\n",
    "    # other code\n",
    "```\n",
    "\n",
    "Now, as soon as we construct a literal, instead of getting a tuple we will get a *function* that returns the set that `matchset()` would have given us. We can then apply this function to `text`.\n",
    "\n",
    "```python\n",
    "def lit(s):\n",
    "    return lambda text: set([text[len(s):]]) if text.startswith(s) else null\n",
    "```\n",
    "\n",
    "## Lower Level Compilers\n",
    "\n",
    "We can define a pattern, say `pat = lit('a')` which is now a function, not a tuple, which gives us the set of the remainders. In an interpreter we have patterns that describe the strings, i.e. the language. In a compiler we have two sets of descriptions to deal with: a description of what the pattern looks like and a description for what the compiled code looks like. In our case, the compiled code consists of Python functions, which are a good target representation because they are flexible. Compilers for languages like C generate code that is the actual machine instructions for the computer, and this is a complex process. There is an intermediate process that generates code for a Virtual Machine. Java and Python follow this approach. The `dis(code)` function from the `dis` module generates the *bytecode* for the Python virtual machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4           0 RESUME                   0\n",
      "              2 LOAD_GLOBAL              0 (math)\n",
      "             14 LOAD_METHOD              1 (sqrt)\n",
      "             36 LOAD_FAST                0 (x)\n",
      "             38 LOAD_CONST               1 (2)\n",
      "             40 BINARY_OP                8 (**)\n",
      "             44 LOAD_FAST                1 (y)\n",
      "             46 LOAD_CONST               1 (2)\n",
      "             48 BINARY_OP                8 (**)\n",
      "             52 BINARY_OP                0 (+)\n",
      "             56 PRECALL                  1\n",
      "             60 CALL                     1\n",
      "             70 RETURN_VALUE\n"
     ]
    }
   ],
   "source": [
    "import dis\n",
    "import math\n",
    "\n",
    "dis.dis(lambda x, y: math.sqrt(x**2 + y**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below, we implement the compiled functions for `lit(s)`, `seq(x, y)` and `alt(x, y)`. The point is to remember that each of `x, y` are functions returning a set, therefore the compiler for `alt(x, y)` is the union of `x(text)`, which is a set, and `y(text)`, which is another set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test passes\n"
     ]
    }
   ],
   "source": [
    "def lit(s):\n",
    "    return lambda text: set([text[len(s):]]) if text.startswith(s) else null\n",
    "\n",
    "def seq(x, y):\n",
    "    return lambda text: set().union(*map(y, x(text)))\n",
    "\n",
    "def alt(x, y):\n",
    "    return lambda text: x(text).union(y(text))\n",
    "        \n",
    "null = frozenset([])\n",
    "\n",
    "def oneof(chars):\n",
    "    return lambda t: set([t[1:]]) if (t and t[0] in chars) else null\n",
    "\n",
    "def star(x): return lambda t: (set([t]) | \n",
    "                               set(t2 for t1 in x(t) if t1 != t\n",
    "                                   for t2 in star(x)(t1)))\n",
    "\n",
    "dot = lambda t: set([t[1:]]) if t else null\n",
    "eol = lambda t: set(['']) if t == '' else null\n",
    "\n",
    "def test():\n",
    "    g = alt(lit('a'), lit('b'))\n",
    "    assert g('abc') == set(['bc'])\n",
    "    return 'test passes'\n",
    "\n",
    "print(test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this approach, `match(pattern, text)` can be written as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests pass\n"
     ]
    }
   ],
   "source": [
    "def match(pattern, text):\n",
    "    \"Match pattern against start of text; return longest match found or None.\"\n",
    "    remainders = pattern(text)\n",
    "    if remainders:\n",
    "        shortest = min(remainders, key=len)\n",
    "        return text[:len(text)-len(shortest)]\n",
    "    \n",
    "def test():\n",
    "    assert match(star(lit('a')), 'aaaaabbbaa') == 'aaaaa'\n",
    "    assert match(lit('hello'), 'hello how are you?') == 'hello'\n",
    "    assert match(lit('x'), 'hello how are you?') == None\n",
    "    assert match(oneof('xyz'), 'x**2 + y**2 = r**2') == 'x'\n",
    "    assert match(oneof('xyz'), '   x is here!') == None\n",
    "    return 'tests pass'\n",
    "\n",
    "print(test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recognizers and Generators\n",
    "\n",
    "What we have done so far is the **recognizer task**. We have a function `match(pattern, text)` which *recognizes* if the prefix of `text` is in the language defined by `pattern`.\n",
    "\n",
    "The **generator task** takes a pattern `pattern` and generates the complete language defined by that pattern. For example, the pattern `(a|b)(a|b)` generates the language `{'aa', 'ab', 'ba', 'bb}`. If the pattern is `a*` the corresponding language is an infinite set. We could use a generator function to generate each element of such language one at a time, but we will instead limit the sizes of the strings we want, and this will always produce finite sets. We will take the compiler's approach, and instead of calling `gen(pat)`, i.e., the generator, as a function, on the pattern, we will have the generator compiled into the pattern. `pat()` will therefore be a function and we will apply it to a set of integers representing the possible range of lengths that we want to retrieve and that will return a set of strings. For example, given `pat = a*`, `pat({1, 2, 3})` should return all strings of length 1, 2, or 3, i.e., `{'a', 'aa', 'aaa'}`. The functions below implement the generators for the various operations.\n",
    "\n",
    "This is the whole compiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tests pass\n"
     ]
    }
   ],
   "source": [
    "def lit(s):\n",
    "    return lambda Ns: set([s]) if len(s) in Ns else null\n",
    "\n",
    "def alt(x, y):\n",
    "    return lambda Ns: x(Ns) | y(Ns)\n",
    "\n",
    "def star(x):\n",
    "    return lambda Ns: opt(plus(x))(Ns)\n",
    "\n",
    "def plus(x):\n",
    "    return lambda Ns: genseq(x, star(x), Ns, startx=1) #Tricky\n",
    "\n",
    "def oneof(chars):\n",
    "    return lambda Ns: set(chars) if 1 in Ns else null\n",
    "\n",
    "def seq(x, y):\n",
    "    return lambda Ns: genseq(x, y, Ns)\n",
    "\n",
    "def opt(x):\n",
    "    return alt(epsilon, x)\n",
    "\n",
    "dot = oneof('?')    # You could expand the alphabet to more chars.\n",
    "epsilon = lit('')   # The pattern that matches the empty string.\n",
    "\n",
    "def test():\n",
    "    \n",
    "    f = lit('hello')\n",
    "    assert f(set([1, 2, 3, 4, 5])) == set(['hello'])\n",
    "    assert f(set([1, 2, 3, 4]))    == null \n",
    "    \n",
    "    g = alt(lit('hi'), lit('bye'))\n",
    "    assert g(set([1, 2, 3, 4, 5, 6])) == set(['bye', 'hi'])\n",
    "    assert g(set([1, 3, 5])) == set(['bye'])\n",
    "    \n",
    "    h = oneof('theseletters')\n",
    "    assert h(set([1, 2, 3])) == set(['t', 'h', 'e', 's', 'l', 'r'])\n",
    "    assert h(set([2, 3, 4])) == null\n",
    "    \n",
    "    return 'tests pass'\n",
    "\n",
    "print(test())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the compiler more efficient. For example, in the definition of `lit(s)` we call `set([s])` every time the output of `lit(s)` is called. This seems wasteful. A better way of writing it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lit(s):\n",
    "    set_s = set([s])  # We create this only once\n",
    "    # Every time we call the function below, we refer to the set_s defined above\n",
    "    return lambda Ns: set_s if len(s) in Ns else null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can pull out the `set(chars)` in the defintion of `oneof(chars)`.\n",
    "\n",
    "We still must define `genseq()`. If we pass two arguments, `x, y` to `seq(x, y)` (not `genseq()`), this returns a function of `Ns`, `fn(Ns)` which returns a set of texts that match. In this respect, `seq(x, y)` is delaying the computation of the output. `geneseq(x, y, Ns)`, instead, immediately calculates the output set. One thing we know about this function is that we will have to call `x(Nx)`, where `Nx` is a set of numbers which we don't yet know, and then we will have to call `y(Ny)`, where `Ny` is a possibly different set of numbers, then we have to concatenate together the results and see if this concatenation is within the allowable set defined by `Ns`. What do we know about `Nx` and `Ny` with respect to `Ns`? `Ns` could be a dense set, say `{0, 1, 2, ..., 10}` or it could be a sparse set, say just `{10}`, but in either case, `Nx + Ny <= 10`, and `Nx` can be anything up to 10. For the `y(Ny)` we have two choices: we could wait for `x(Nx)` to return its results and pass them through `y` or we could do it all at once and then try to combine them together and see if they match up. This is easier because in such case `Ny` could also be any number up to 10 in our example. So, both `Nx` and `Ny` could be anything up to 10 inclusive and if we get some results out, for each of them we add them up and check if they are in `Ns`. A candidate solution for the `geneseq()` function is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def genseq(x, y, Ns):\n",
    "    Nss = range(max(Ns) + 1)\n",
    "    return set(m1 + m2\n",
    "               for m1 in x(Nss)\n",
    "               for m2 in y(Nss)\n",
    "               if len(m1 + m2) in Ns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function, however, can give rise to infinite recursions. Where do we use recursion? In two functions:`plus()` and `star()`, but `star()` is defined in terms `plus()`, so we need to fix `plus()` in order to avoid infinite recursion. We are essentially defining `x+` as `xx*`, i.e., `seq(x, (star, x))`. In most cases, this works, but if we define `pat = plus(opt(a))`, `opt(a)` means that we are picking either `a` or the empty string, and as we go through the loop we may pick the empty string an infinite number of times and we are never going to get past the values in the set `Ns`, and we will keep going forever. **TODO**: clarify this mess.\n",
    "\n",
    "This is why we have `startx=1` in `star()`, i.e., we always ask `x` to have a length of at least 1, and this is how we break the recursion. We redefine `geneseq()` as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_gen passes\n"
     ]
    }
   ],
   "source": [
    "def genseq(x, y, Ns, startx=0):\n",
    "    \"Set of matches to xy whose total len is in Ns, with x-match's len in Ns-len(...\"\n",
    "    # Tricky part: x+ is defined as x+ = x x*\n",
    "    # To stop the recursion, the first x must generate at least 1 char,\n",
    "    # and then the recursive x* has that many fewer characters. We use\n",
    "    # startx=1 to say that x must match at least 1 character.\n",
    "    if not Ns:\n",
    "        return null\n",
    "    xmatches = x(set(range(startx, max(Ns) + 1)))\n",
    "    Ns_x = set(len(m) for m in xmatches)\n",
    "    Ns_y = set(n - m for n in Ns for m in Ns_x if n - m >= 0)\n",
    "    ymatches = y(Ns_y)\n",
    "    return set(m1 + m2\n",
    "               for m1 in xmatches for m2 in ymatches\n",
    "               if len(m1 + m2) in Ns)\n",
    "\n",
    "def test_gen():\n",
    "    def N(hi):\n",
    "        return set(range(hi + 1))\n",
    "    a, b, c = map(lit, 'abc')\n",
    "    assert star(oneof('ab'))(N(2)) == set(['', 'a', 'aa', 'ab', 'ba', 'bb', 'b'])\n",
    "    assert (seq(star(a), seq(star(b), star(c)))(set([4])) == set(\n",
    "        ['aaaa', 'aaab', 'aaac', 'aabb', 'aabc', 'aacc', 'abbb', 'abbc',\n",
    "         'abcc', 'accc', 'bbbb', 'bbbc', 'bbcc', 'bccc', 'cccc']))\n",
    "    assert (seq(plus(a), seq(plus(b), plus(c)))(set([5])) == set(\n",
    "        ['aaabc', 'aabbc', 'aabcc', 'abbbc', 'abbcc', 'abccc']))\n",
    "    assert (seq(oneof('bcfhrsm'), lit('at'))(N(3)) == set(\n",
    "        ['bat', 'cat', 'fat', 'hat', 'mat', 'rat', 'sat']))\n",
    "    assert (seq(star(alt(a, b)), opt(c))(set([3])) == set(\n",
    "        ['aaa', 'aab', 'aac', 'aba', 'abb', 'abc', 'baa', 'bab',\n",
    "         'bac', 'bba', 'bbb', 'bbc']))\n",
    "    assert lit('hello')(set([5])) == set(['hello'])\n",
    "    assert lit('hello')(set([4])) == set()\n",
    "    assert lit('hello')(set([6])) == set()\n",
    "    return 'test_gen passes'\n",
    "\n",
    "print(test_gen())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all the above we have taken advantage of the *composability* of Python functions. Functions, unlike statements and expressions, which can only be composed by the programmer, can be composed dynamically. Functions also provide *control over time*: we can divide some of the work we want to do such that we do some now and some later. Expressions don't allow this separation.\n",
    "\n",
    "## Changing `seq()`\n",
    "\n",
    "The `seq()` function is binary, in the sense that it takes two arguments. If we want a sequence of four objects, say `a, b, c, d`, we need to call `seq(a, seq(b, seq(c, d)))`. It would be much easier if we could just write `seq(a, b, c, d)`. We want to refactor this function, but aren't we changing it's API? We should ask ourselves:\n",
    "\n",
    "1. Which other functions does `seq()` interact with in our program?\n",
    "2. If I change `seq()`, are these changes *backward compatible*? In other words, do I have to modify also the functions `seq()` interacts with?\n",
    "3. Are the changes *internal* or *external*? Am I changing something inside `seq()` that doesn't affect the callers, or am I changing the interface to the outside world?\n",
    "\n",
    "### Function mapping: decorators\n",
    "\n",
    "We can refactor `seq()` without changing the API. To do this, we need to *map* our binary function `f(x, y)` and convert it to an n-ary function `g(x, y, ...)`. This mapping is done via a function, `n_ary()` in the example below, that takes the binary function `f(x, y)` and returns an n-ary function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('myseq', 'a', ('myseq', 'b', 'c'))\n"
     ]
    }
   ],
   "source": [
    "def n_ary(f):\n",
    "    \"\"\"Given binary function f(x, y), return an n_ary function such that\n",
    "    f(x, y, z) = f(x, f(y, z)) etc. Also allow for f(x) = x.\"\"\"\n",
    "    def n_ary_f(x, *args):\n",
    "        return x if not args else f(x, n_ary_f(*args))\n",
    "    return n_ary_f\n",
    "\n",
    "def myseq(x, y):\n",
    "    \"My own seq function (meh)\"\n",
    "    return ('myseq', x, y)\n",
    "\n",
    "myseq = n_ary(myseq)\n",
    "print(myseq('a', 'b', 'c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pattern above is so common in Python that there is a special notation: the **decorator notation**. We can leverage it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('myseq', 'a', ('myseq', 'b', 'c'))\n"
     ]
    }
   ],
   "source": [
    "@n_ary\n",
    "def myseq(x, y):\n",
    "    \"My own seq function (meh)\"\n",
    "    return ('myseq', x, y)\n",
    "\n",
    "print(myseq('a', 'b', 'c'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One limitation, however, is that if we check the docstring of the decorated function, this is what we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function n_ary_f in module __main__:\n",
      "\n",
      "n_ary_f(x, *args)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(myseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily `functools` has a function called `update_wrapper()` that takes two functions and copies the name, the documentation plus other things from the old function to the new function. For this, we need to modify the definition of `n_ary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import update_wrapper\n",
    "\n",
    "def n_ary(f):\n",
    "    \"\"\"Given binary function f(x, y), return an n_ary function such that\n",
    "    f(x, y, z) = f(x, f(y, z)) etc. Also allow for f(x) = x.\"\"\"\n",
    "    def n_ary_f(x, *args):\n",
    "        return x if not args else f(x, n_ary_f(*args))\n",
    "    update_wrapper(n_ary_f, f)  # update_wrapper(new_fn, old_fn) \n",
    "    return n_ary_f\n",
    "\n",
    "@n_ary\n",
    "def myseq(x, y):\n",
    "    \"My own seq function (meh)\"\n",
    "    return ('myseq', x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function myseq in module __main__:\n",
      "\n",
      "myseq(x, y)\n",
      "    My own seq function (meh)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(myseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An even better approach is to create our own decorator that adds the the `update_wrapper()` call to, in the case above, `n_ary_f()`. We will call this new decorator `@decorator`. If we use `n_ary()` as a decorator on `myseq()` and apply `@decorator` to `n_ary()`, we have two updates to consider: one for the function we want to decorate, and one for the decorator itself. The pattern we want to follow is:\n",
    "\n",
    "```python\n",
    "def decorator(d):  # d is a decorator function\n",
    "    def _d(f):\n",
    "        update_wrapper(d(f), f)\n",
    "    update_wrapper(_d, d)\n",
    "    return _d\n",
    "```\n",
    "\n",
    "With this setup, `n_ary = decorator(n_ary)` would be updated by `update_wrapper(_d, d)` and `myseq = n_ary(myseq)` would be updated by `update_wrapper(d(f), f)`.\n",
    "\n",
    "This is what we ultimately get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorator(d):\n",
    "    \"Make function d a decorator. d wraps a function f\"\n",
    "    def _d(f):\n",
    "        return update_wrapper(d(f), f)\n",
    "    update_wrapper(_d, d)\n",
    "    return _d\n",
    "\n",
    "@decorator\n",
    "def n_ary(f):\n",
    "    \"\"\"Given bynary function f(x, y) return an n-ary function such that\n",
    "    f(x, y ,z) = f(x, f(y, z)), etc. Also allow for f(x) = x.\"\"\"\n",
    "    def n_ary_f(x, *args):\n",
    "        return x if not args else f(x, n_ary_f(*args))\n",
    "    return n_ary_f\n",
    "\n",
    "@decorator\n",
    "def myseq(x, y):\n",
    "    return ('myseq', x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function myseq in module __main__:\n",
      "\n",
      "myseq(x, y)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(myseq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even more confusingly, the following code also works (due to [Darius Bacon](https://github.com/darius)). **TODO** understand what's going on here. Video 46 - Decorated Decorators Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decorator(d):\n",
    "    \"Make function d a decorator. d wraps a function fn.\"\n",
    "    return lambda fn: update_wrapper(d(fn), fn)\n",
    "\n",
    "decorator = decorator(decorator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cache Management\n",
    "\n",
    "We want to leverage the concept of **memoization**. Particularly with recursive functions we will be making the same function calls over and over again. If the result of those function calls does not change, and it takes a long time to be computed, it is better to store the input and the relative result in a cache. For example:\n",
    "\n",
    "```python\n",
    "def fib(n):\n",
    "    if n in in cache:\n",
    "        return cache[n]\n",
    "    cache[n] = result = # code to compute the result\n",
    "    return result\n",
    "```\n",
    "\n",
    "However, we may have many functions where we may want to use memoization, and we don't want to rewrite the code above over and over. We can implement this with a decorator, let's call it `@memo`. It looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@decorator\n",
    "def memo(f):\n",
    "    \"\"\"Decorator that caches the return value for each call to f(args).\n",
    "    Then, when called again with same args, we can just look it up.\"\"\"\n",
    "    cache = {}\n",
    "    def _f(*args):\n",
    "        try:\n",
    "            return cache[args]\n",
    "        except KeyError:\n",
    "            cache[args] = result = f(*args)\n",
    "            return result\n",
    "        except TypeError:\n",
    "            # Some elements of args can't be a dict key\n",
    "            return f(args)\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have use a `try-except` pattern rather than an `if-else` one. It's like asking for forgiveness (`try-catch`) as opposed to asking for permissiono (`if-else`). In this case we use the `try-except` because we have the second type of exception: `TypeError`, which happens when the argument is not hashable, for example if we use a list as a key. If we used a particularly simple hash function for lists of integers, say the sum of the elements, we may have `y = [1, 2, 3]` which would be associated with the hash value 6. If, however, we modify the list so that `y[0] = 10`, now the hash value is 15.\n",
    "\n",
    "To see how effective our `@memo` decorator is we may compare the decorated version of the function with the original one. We may measure time or, more interestingly, the number of function calls. We did something like this in a previous lesson, but now we will do it with a decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@decorator\n",
    "def countcalls(f):\n",
    "    \"Decorator that makes the function count calls to it, in callcounts[f].\"\n",
    "    def _f(*args):\n",
    "        callcounts[_f] +=1\n",
    "        return f(*args)\n",
    "    callcounts[_f] = 0\n",
    "    return _f\n",
    "\n",
    "callcounts = {}\n",
    "\n",
    "@countcalls\n",
    "def fib(n):\n",
    "    return 1 if n <= 1 else fib(n - 1) + fib(n - 2)\n",
    "\n",
    "fib(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@countcalls\n",
    "@memo\n",
    "def fib(n):\n",
    "    return 1 if n <= 1 else fib(n - 1) + fib(n - 2)\n",
    "\n",
    "fib(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** this testing function is currently useless. It already treats the API calls as functions rather as than tuples above.\n",
    "\n",
    "One fascinating thing is that, in the case of the function *without* memoization, if we compute the number of function calls for `fib(n)`, let's call it `calls[n]`, and divide by the number of calls for `fib(n-1)`, let's call it `calls[n-1]`, this ratio tends to the golden ratio $(1 + \\sqrt{5})/2$ for n tending to infinity.\n",
    "\n",
    "## Trace Tool\n",
    "\n",
    "We have seen three main types of tools:\n",
    "\n",
    "1. Debugging tools: `countcalls()`\n",
    "2. Performance tools: `memo()`.\n",
    "3. Expressiveness tools: `n_ary()`\n",
    "\n",
    "Expressiveness tools give you more power to say more about your language. Performance tools make things faster. We want to add a debugging tool called `trace()` that helps debugging functions. In our Fibonacci example, every time we make a call we indent to the right. Every time we return we de-indent to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "@decorator\n",
    "def trace(f):\n",
    "    def _f(*args):\n",
    "        signature = '%s(%s)' % (f.__name__, ', '.join(map(repr, args)))\n",
    "        print('%s--> %s' % (trace.level * indent, signature))\n",
    "        trace.level += 1\n",
    "        try:\n",
    "            result = f(*args)\n",
    "            print('%s<-- %s === %s' % ((trace.level - 1) * indent, signature, result))\n",
    "        finally:\n",
    "            # If the function returns an error, we want to make sure we decrement.\n",
    "            trace.level -= 1\n",
    "        return result\n",
    "    trace.level = 0\n",
    "    return _f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disable Decorator\n",
    "\n",
    "We want to have one more debug tool that we will call `disabled()`. This is just the identity function, and the idea is to deactivate some of the decorators we may have used. If we redefine `trace = disabled` and reload our program, and now `@trace` will return the function itself.\n",
    "\n",
    "## Back To Languages\n",
    "\n",
    "Imagine we want to manipulate algebraic expressions like `y + (3 + x)`. Regular expressions can handle a fixed number of parenthesized expressions, but not an arbitrary number of them (the problem is making sure that right parentheses match left parentheses). We will need **context free languages** to solve this problem.\n",
    "If I have an expression like `(m * x) + b` we want to make sure that `x` is parsed together with `m` and not together with `b`.  In other words, we want to be able to write expressions like `... + ... + ...`, where multiplications are part of the `...`. We refer to the `...` parts as to **terms**. We want to write a grammar that defines the language of these expressions. Remember that the grammar is the *description* and the language is the set of all possible strings described by the grammar. \n",
    "\n",
    "In general we say that an expression consists of a term followed by an operator (for simplicity say only '+/-') and another expression, or `Exp => Term[-+]Exp|Term` The `|Term` part covers the case where our whole expression consists of one single term. This is the base case of our recursive definition of expression. \n",
    "```\n",
    "Expr => Term[-+]Expr | Term\n",
    "```\n",
    "\n",
    "The rule for a term is similar.\n",
    "\n",
    "```\n",
    "Term[*/]\n",
    "```\n",
    "\n",
    "- a: this is an expression consisting of one term, a\n",
    "- a + b: this is an expression consisting of two terms.\n",
    "- a + b + c: this expression consists of three terms.\n",
    "\n",
    "We can then write the rule for a term, taking into account the operations etc. Norvig writes down the following as the complete grammar for our language. We don't have something to interpret it, but we are operating on a \"wishful thinking\" basis.\n",
    "\n",
    "```text\n",
    "Exp => Term [+-] Exp | Term\n",
    "Term => Factor [*/] Term | Factor\n",
    "Factor => Funcall | Var | Num | [(] Exp [)]\n",
    "Funcall => Var[(] Exps [)]\n",
    "Exps => Exp [,] Exps | Exp\n",
    "Var => [a-zA-Z_]\\w*\n",
    "Num => [-+]?[0-9]+([.][0-9]*)?\n",
    "```\n",
    "\n",
    "Now we need a function that can take the string above and use it in something we can use as a parser, ideally a function `G = grammar(...)` where `...` is the set of rules above, represented as a string. What would be a good format for `G`? A dictionary, the keys of which would be the names on the l.h.s. of the rules, and the values would be some object corresponding to the representation on the r.h.s. Norvig's choice is to use tuples of possible choices. He uses tuples and not sets because order matters. Each element of the tuple is going to be a sequence, more precisely a list, and each element of the list is going to be an atom, where an atom can be either a name of the categories in the rules above, or a regular expression matching them. It would look something like\n",
    "\n",
    "```python\n",
    "G = {'Exp': (['Term', '[+-]', 'Exp'], ['Term']),\n",
    "     'Term': (['Factor', '[*/]', 'Term'], ['Factor'])}\n",
    "```\n",
    "\n",
    "We could have written the grammar as a dictionary from the beginning, but the idea here is to first imagine the language as we wish it was, and then and only then write it in a way the computer can understand. All we have to do is write the function `Grammar` that converts the string of rules into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = r\"\"\"\n",
    "Exp     => Term [+-] Exp | Term\n",
    "Term    => Factor [*/] Term | Factor\n",
    "Factor  => Funcall | Var | Num | [(] Exp [)]\n",
    "Funcall => Var[(] Exps [)]\n",
    "Exps    => Exp [,] Exps | Exp\n",
    "Var     => [a-zA-Z_]\\w*\n",
    "Num     => [-+]?[0-9]+([.][0-9]*)?\n",
    "\"\"\"\n",
    "\n",
    "def split(text, sep=None, maxsplit=-1):\n",
    "    \"Like str.split applied to text, but strips whitespace from each piece.\"\n",
    "    return [t.strip() for t in text.strip().split(sep, maxsplit) if t]\n",
    "\n",
    "def grammar(description):\n",
    "    \"\"\"Convert a description to a grammar.\"\"\"\n",
    "    G = {}\n",
    "    for line in split(description, '\\n'):\n",
    "        lhs, rhs = split(line, ' => ', 1)\n",
    "        alternatives = split(rhs, ' | ')\n",
    "        G[lhs] = tuple(map(split, alternatives))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Exp': (['Term', '[+-]', 'Exp'], ['Term']),\n",
       " 'Term': (['Factor', '[*/]', 'Term'], ['Factor']),\n",
       " 'Factor': (['Funcall'], ['Var'], ['Num'], ['[(]', 'Exp', '[)]']),\n",
       " 'Funcall': (['Var[(]', 'Exps', '[)]'],),\n",
       " 'Exps': (['Exp', '[,]', 'Exps'], ['Exp']),\n",
       " 'Var': (['[a-zA-Z_]\\\\w*'],),\n",
       " 'Num': (['[-+]?[0-9]+([.][0-9]*)?'],)}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = grammar(description)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grammar so defined would parse an expression like `m*x+b` correctly, but would fail on `m * x + b`. The following version of `grammar()` allows for spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammar(description, whitespace=r'\\s*'):\n",
    "    \"\"\"Convert a description to a grammar. Each line is a rule for a\n",
    "    non-terminal symbol; it looks like this:\n",
    "        Symbol => A1 A2 ... | B1 B2 ... | C1 C2...\n",
    "    where the right-hand side is one or more alternatives, separated by spaces.\n",
    "    An atom is either a symbol on some left-hand side, or it is a regular\n",
    "    expression that will be passed to re.match to match a token.\n",
    "    Notation for *, +, or ? not allowed in a rule alternative (but ok within\n",
    "    a token). Use '\\' to continue long lines. You must include spaces or tabs\n",
    "    around '=>' and '|'. That's within the grammar description itself.\n",
    "    The grammar that gets defined allows whitespace betwen tokens by default.\n",
    "    Specify '' as the second argument to grammar() to disallow this (or supply\n",
    "    any regular expression to describe allowable whitespace between tookens)\"\"\"\n",
    "    G = {' ': whitespace}\n",
    "    description = description.replace('\\t', ' ') # Replace tabs with spaces\n",
    "    for line in split(description, '\\n'):\n",
    "        lhs, rhs = split(line, ' => ', 1)\n",
    "        alternatives = split(rhs, ' | ')\n",
    "        G[lhs] = tuple(map(split, alternatives))\n",
    "    return G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser\n",
    "\n",
    "We now need a parser, i.e., a function `parse(symbol, text, G)`. `text` is the text we want to parse and `G` is the grammar. This function returns a single result, i.e., a single remainder and not a set of remainders as in the previous case. This to make the grammar unambiguous. For example, if we want to pars an `Exp` we want first to see if we can parse the first alternative (remember, `Exp => Term [+-] Exp | Term`). If we can, we don't look at the alternative. We read rules from left to right, this is why earlier we said that order matters. If we wrote `Exp => Term | Term [+-] Exp` and we tried to parse `a + 3` the parser would stop at `a` and ignore the rest.\n",
    "\n",
    "Previously we used regular expressions as *recognizers*. Here we use them as **parsers**, where they recognize whether an expression is part of the language, but they also provide an internal structure, the parse tree. Ultimately `parse(symbol, text, G)` will return a tuple with the parse tree and the remainder. If we parse\n",
    "\n",
    "```python\n",
    "parse('Exp', 'a * x', G)\n",
    "```\n",
    "\n",
    "we want to get back\n",
    "\n",
    "```python\n",
    "(['Exp', ['Term', ['Factor', ['Var', 'a']], '*', ['Term', ['Factor', ['Var', 'x']]]]],\n",
    " '')\n",
    "```\n",
    "\n",
    "The first row is the parse tree, the second row, containing only the empty string, is the remainder, which is emtpy because we consumed the whole expression. We will assume that failure returns the tuple `(None, None)`.\n",
    "\n",
    "There are four cases we need to parse. We must be able to parse\n",
    "\n",
    "1. An expression: `Exp`.\n",
    "2. A regular expression: `[+-]`.\n",
    "3. Alternatives: `([...], [...])`.\n",
    "4. A list of atoms representing a sequence: `[..., ..., ...]`.\n",
    "\n",
    "We will associate the following functions with the 4 cases above.\n",
    "\n",
    "1. `parse_atom()`.\n",
    "2. A variable `tokenizer`.\n",
    "3. Included in `parse_atom()`.\n",
    "4. `parse_sequence()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer has two jobs: first, it has to handle whitespace that occurs before the token.\n",
    "`parse_sequence`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def parse(start_symbol, text, grammar):\n",
    "    \"\"\"Example call: parse('Exp', '3*x + b', G).\n",
    "    Returns a (tree, remainder) pair. If remainder is '', it parsed the whole\n",
    "    string. Failure iff remainder is None. This is a deterministic PEG parser,\n",
    "    so rule order (left-to-right) matters. Do 'e => T op E | T', putting the\n",
    "    longest parse first; don't do 'E =? T | T op E'\n",
    "    Also, no left recursion allowed: don't do 'E => E op T'\"\"\"\n",
    "    tokenizer = grammar[' '] + '(%s)'\n",
    "\n",
    "    def parse_sequence(sequence, text):\n",
    "        result = []\n",
    "        for atom in sequence:\n",
    "            tree, text = parse_atom(atom, text)\n",
    "            if text is None:\n",
    "                return Fail\n",
    "            result.append(tree)\n",
    "        return result, text\n",
    "\n",
    "    @memo\n",
    "    def parse_atom(atom, text):\n",
    "        if atom in grammar:  # Non-terminal: tuple of alternatives\n",
    "            for alternative in grammar[atom]:\n",
    "                tree, rem = parse_sequence(alternative, text)\n",
    "                if rem is not None:\n",
    "                    return [atom] + tree, rem\n",
    "            return Fail\n",
    "        else:  # Terminal: match characters againast start of text\n",
    "            m = re.match(tokenizer % atom, text)\n",
    "            return Fail if (not m) else (m.group(1), text[m.end():])\n",
    "    \n",
    "    # Body of parse:\n",
    "    return parse_atom(start_symbol, text)\n",
    "\n",
    "Fail = (None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice there is a `@memo` decorator above `parse_atom`. Why? Suppose we have a very long term `(x+y....)` and we parse it using the rule `Exp => Term [+-] Exp | Term`. We parse the long term, then we look for a `[+-]`. If we do not find it, we fall back to the alternative, `Term`, which implies that we have to go back and parse the long term again. This is inefficient. We would like our parser to do this work only once. Adding the `@memo` decorator makes this parser faster.\n",
    "\n",
    "`parse_atom(atom, text)` takes two strings as arguments, and strings are hashable. `parse(start_symbol, text, grammar)`, however, takes also the `grammar` argument, which is not hashable, therefore we cannot memoize the whole `parse()` function.\n",
    "\n",
    "The grammar below is based on the rules defined at [www.w3.org](http://www.w3.org/Addressing/URL/5_BNF.html) and can be used to parse URLs. The `verify()` function finds all the tokens that are on the lhs, on the rhs. `Terminals` are tokens that are on the rhs but not on the lhs. `Suspects` are the tokens that look like they should appear on the lhs but they don't.\n",
    "`Orphans` are tokens that appear on the lhs but not on the rhs, and are therefore useless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = grammar(\n",
    "    \"\"\"\n",
    "url => httpaddress | ftpaddress | mailtoaddress\n",
    "httpaddress => http:// hostport /path? ?search?\n",
    "ftpaddress => ftp:// login /path ; ftptype | ftp:// login / path\n",
    "/path? => path | ()\n",
    "?search? => [?] search | ()\n",
    "mailtoaddress => mailto: xalphas @ hostname\n",
    "hostport => host : port | host\n",
    "host => hostname | hostnumber\n",
    "hostname => ialpha . hostname | ialpha\n",
    "hostnumber => digits . digits . digits . digits\n",
    "ftptype => A formcode | E formcode | I | L digits\n",
    "formcode => [NTC]\n",
    "port => digits | path\n",
    "path => void | segment / path | segment\n",
    "segment => xalphas\n",
    "search => xalphas + search | xalphas\n",
    "login => userpassword hostport | hostport\n",
    "userpassword => user : password @ | user @\n",
    "user => alphanum2 user | alphanum2\n",
    "password => alphanum2 password | password\n",
    "path => void | segment / path | segment\n",
    "void => ()\n",
    "digits => digits digits | digit\n",
    "digit => [0-9]\n",
    "alpha => [a-zA-Z]\n",
    "safe => [-$_@.&+]\n",
    "extra => [()!*''\"\"]\n",
    "escape => % hex hex\n",
    "hex => [0-9a-fA-F]\n",
    "alphanum => alpha | digit\n",
    "alphanums => alpha | digit | [-_.+]\n",
    "ialpha => alpha xalphas | alpha\n",
    "xalphas => xalpha xalphas | xalpha\n",
    "xalpha => alpha | digit | safe | extra | escape\n",
    "\"\"\", whitespace='()')\n",
    "\n",
    "# Helper function to verify grammars\n",
    "def verify(G):\n",
    "    lhstokens = set(G) - set([' '])\n",
    "    rhstokens = set(t for alts in G.values() for alt in alts for t in alt)\n",
    "    def show(title, tokens):\n",
    "        print(title, '=', ' '.join(sorted(tokens)))\n",
    "    show('Non-Terms', G)\n",
    "    show('Terminals', rhstokens - lhstokens)\n",
    "    show('Suspects ', [t for t in (rhstokens - lhstokens) if t.isalnum()])\n",
    "    show('Orphans ', lhstokens - rhstokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Terms =   /path? ?search? alpha alphanum alphanums digit digits escape extra formcode ftpaddress ftptype hex host hostname hostnumber hostport httpaddress ialpha login mailtoaddress password path port safe search segment url user userpassword void xalpha xalphas\n",
      "Terminals = % ( () ) + . / /path : ; @ A E I L [()!*''\"\"] [-$_@.&+] [-_.+] [0-9] [0-9a-fA-F] [?] [NTC] [a-zA-Z] alphanum2 ftp:// http:// mailto:\n",
      "Suspects  = A E I L alphanum2\n",
      "Orphans  = alphanum alphanums url\n"
     ]
    }
   ],
   "source": [
    "verify(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This unit was about tools, how to build useful tools, how to apply them to components of a domain. We focused on one particular tool: language. We explored the ability to define our own language rather than rely on what Python makes available to us. We talked about grammars, interpreters, and compilers. The other tool we explored are functions. We saw that functions are very powerful, in ways statements cannot be, and this is because statements are not as composable as functions are. If you want to reuse a statement the only thing you can do is copy&paste, and this is a problem when you need to update a statement. With functions we do not have this limitation because we can compose them, rather than copy&paste them. When you modify the a function it is automatically modified wherever it is called. We talked about decorators as functions, about functions as objects, and we showed some patterns on how to put them together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - JSON parser\n",
    "\n",
    "The homework requires writing a grammar for the JSON language. You can look at [json.org](http://www.json.org). There is a little grammar on the right hand side, but it is not in the format we expect, so it needs to be translated into it. You should be able to parse the top level of the text, called `value`\n",
    "\n",
    "```python\n",
    "JSON = grammar(\n",
    "    ## your description here\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_parse(text):\n",
    "    return parse('value', text, JSON)\n",
    "\n",
    "def json_test():\n",
    "    for e in examples:\n",
    "        print(e, ' == ', parse('value', e, JSON))\n",
    "\n",
    "examples = ['[\"testing\", 1, 2, 3]',\n",
    "            '-123.456e+789',\n",
    "            '{\"age\": 21, \"state\": \"CO\", \"occupation\": \"rides the rodeo\"}']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON = grammar(\n",
    "\"\"\"\n",
    "object => {} | { members }\n",
    "members => pair, members | pair\n",
    "pair => string : value\n",
    "array => [[] []] | [[] elements []]\n",
    "elements => value, elements | value\n",
    "value => string | number | object | array | true | false | null\n",
    "string => \"[^\"]*\"\n",
    "number => int frac exp | int frac | int exp | int\n",
    "int => -?[1-9][0-9]*\n",
    "frac => [.][0-9]+\n",
    "exp => [eE][-+]?[0-9]+\n",
    "\"\"\", whitespace='\\*s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework - Inverse Function\n",
    "\n",
    "We want to be able to compute inverse functions, and we want to be able to do this only once, and not for each function. For example, it is straightforward to write a function to compute the square of a number using only elementary operations (multiplication), but writing a function that computes the square root using only elementary operations is a lot harder (Newton's method). We would like to be able to write just `sqrt = inverse(square)` and be done with it. We are going ot consider only functions defined on the non-negative numbers and are monotonically increasing. Here is a simple definition, and the goal is to write a more efficient version. The final version should have a runtime closer to the logarithm of the input to `f_1`. Two hints for this algorithm\n",
    "\n",
    "1. Binary search.\n",
    "2. Newton's method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n"
     ]
    }
   ],
   "source": [
    "def inverse(f, delta=1/128.):\n",
    "    \"\"\"Given a function y = f(x) that is a monotonically increasing function on\n",
    "    non-negative numbers, return the fuynction x = f_1(y) that is an approximate\n",
    "    inverse, picking the closest value to the inverse within delta.\"\"\"\n",
    "    def f_1(y):\n",
    "        x = 0\n",
    "        while f(x) < y:\n",
    "            x += delta\n",
    "        # Now x is too big, x-delta is too small; pick the closest to y\n",
    "        return x if (f(x) - y < y - f(x - delta)) else x - delta\n",
    "    return f_1\n",
    "\n",
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "sqrt = inverse(square)\n",
    "\n",
    "print(sqrt(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "We start with a certain step on x. If f(x) i greater, we double the step and we keep doubling until we overshoot f(x), this gives us `low` and `high`. We now use binary search to zero-in on the right value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse(f, delta=1/1024.):\n",
    "    \"\"\"Given a function y = f(x) that is a monotonically increasing function on\n",
    "    non-negative integers, return the function x = f_1(y) that is an approximate\n",
    "    inverse, picking the closest integer to the inverse.\"\"\"\n",
    "    def f_1(y):\n",
    "        lo, hi = find_bounds(f, y)\n",
    "        return binary_search(f, y, lo, hi, delta)\n",
    "    return f_1\n",
    "\n",
    "def find_bounds(f, y):\n",
    "    \"Find values lo, hi such that f(lo) <= y <= f(hi)\"\n",
    "    # Keep doubling x until f(x) >= y; that's hi;\n",
    "    # and lo will be either the previous x or 0.\n",
    "    x = 1.\n",
    "    while f(x) < y:\n",
    "        x = x * 2.\n",
    "    lo = 0 if (x == 1) else x/2.\n",
    "    return lo, x\n",
    "\n",
    "def binary_search(f, y, lo, hi, delta):\n",
    "    \"Given f(lo) <= y <= f(hi), return x such that f(x) is within delta of y.\"\n",
    "    # Continually split the region in half\n",
    "    while lo <= hi:\n",
    "        x = (lo + hi) / 2.\n",
    "        if f(x) < y:\n",
    "            lo = x + delta\n",
    "        elif f(x) > y:\n",
    "            hi = x - delta\n",
    "        else:\n",
    "            return x\n",
    "    return hi if (f(hi) - y < y - f(lo)) else lo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework - Find HTML Tags\n",
    "\n",
    "Finding HTML tags with `str.find()` is not robust to spaces, line-feeds etc. We are looking only for opening tags, not closing ones. Something like `<a href=\"unit3.py\">`, i.e., an opening angle bracket, a tag, optional attributes and a closing bracket. Write a function `findtags(text)` that returns a list of strings. You can use Python's `re` module, our own implementation, the context-free grammar we created, whatever.\n",
    "\n",
    "### Solution\n",
    "\n",
    "The solution below uses the `re` module. For readability we split the regular expression looking for optional attributes and the one looking for tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findtags(text):\n",
    "    parms = '(\\w+\\s*=\\s*\"[^\"]*\"\\s*)*'\n",
    "    tags = '(<\\s*\\w+\\s*' + parms + '\\s*/?>)'\n",
    "    return re.findall(tags, text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem - Grammar and Parser for Regular Expressions\n",
    "\n",
    "Having an API for regular expressions is useful, but the expression `plus(opt(alt(lit('a'), lit('b'))))` can be expressed in strign form as `(a|b)?+`, which is much more concise. The goal of this challenge is to write a grammar and a parser for regular expressions.\n",
    "\n",
    "You should first build a RE grammar using the tools we provided, then a parser `parse('RE', text)` that returns some sort of tree that is not yet the API form, so we need one more function to convert the tree format to the API format.\n",
    "\n",
    "```python\n",
    "REGRAMMAR = grammar(\"\"\"\n",
    "RE => ## your description here\n",
    "\"\"\", whitespace='')\n",
    "\n",
    "def parse_re(pattern):\n",
    "    return convert(parse('RE', pattern, REGRAMMAR))\n",
    "\n",
    "def convert(tree):\n",
    "    ## your code here\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
